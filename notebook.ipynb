{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalação dos modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxNl9Z5OqkR7"
   },
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "# !pip install pymongo[srv]\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8IJZEbYtnYKh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re \n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import pymongo\n",
    "from bson.binary import Binary\n",
    "from bson import ObjectId \n",
    "from pymongo import MongoClient\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEzq2xHenke7",
    "outputId": "fc561f0f-50f9-4c79-aea4-d38cff098ab7"
   },
   "outputs": [],
   "source": [
    "#Só é necessário fazer o download caso não exista a pasta aclImdb_v1 no diretório\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
    "                                    untar=True, cache_dir='.',\n",
    "                                    cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "train_dir = os.path.join(dataset_dir, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gSD_ZYSnyCN"
   },
   "outputs": [],
   "source": [
    "# Essa função é necessária para remover um diretório que não precisa ser usado\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tratando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdGHlfRAn0V8",
    "outputId": "07b7ea74-c7aa-4007-bd96-b184ac742d78"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "#extrai o dataset e separa em 2 sets: Treinamento e Validação\n",
    "#a seed garante que a aleatoriedade não cause overlap na separação\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory('aclImdb/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='training', \n",
    "    seed=seed)\n",
    "\n",
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='validation', \n",
    "    seed=seed)\n",
    "#extrai o dataset para treino\n",
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test',\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definindo primeira camada (padronização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQ4ZqkwvoH9z"
   },
   "outputs": [],
   "source": [
    "#Função que retorna os textos padronizados\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_2rt_QmoIVF"
   },
   "outputs": [],
   "source": [
    "#Usando a função de padronização, é definido a camada que faz a normalização e mapeia as strings para ints(output_mode)\n",
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLLKHPMCoJeS"
   },
   "outputs": [],
   "source": [
    "#Train_text recebe apenas os textos, sem os labels e depois o adapt é chamado pra criar o vocabulário\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXCFmReWoKY2"
   },
   "outputs": [],
   "source": [
    "#Função para vetorizar nossos dados\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ar5qjeizoLW6",
    "outputId": "41117ea2-e61b-4ab8-c53f-c514cb6dffa8"
   },
   "outputs": [],
   "source": [
    "#Testando a vetorização em uma review\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[1], label_batch[1]\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", raw_train_ds.class_names[first_label])\n",
    "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZPcZnIkoNaX"
   },
   "outputs": [],
   "source": [
    "#Armazenando os dados vetorizados\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zukca0klooq-"
   },
   "outputs": [],
   "source": [
    "#Aumento da performance no GPU fazendo prefetching e buffering\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definindo a rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJVtjWE1oplm",
    "outputId": "648802e1-4491-47bd-a230-75a2a2fe8e10"
   },
   "outputs": [],
   "source": [
    "#Adicionando no modelo as camadas\n",
    "embedding_dim = 16\n",
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features + 1, embedding_dim),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1)])\n",
    "  \n",
    "#summary demonstra as informações do nosso modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Z0kXz_BoqoF"
   },
   "outputs": [],
   "source": [
    "#Compilando modelo com Cross Entropy e o adam \n",
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzFmzHEqor2w",
    "outputId": "bc4859b0-58ea-4ed7-a224-5ebf7112954b"
   },
   "outputs": [],
   "source": [
    "#Treinando modelo com 15 epocas\n",
    "epochs = 15\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medindo eficiencia do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3F7D0KUou31",
    "outputId": "4e1a4066-7490-4965-f81e-3fd8d0793016"
   },
   "outputs": [],
   "source": [
    "#Medindo Loss e Acuracia com dados de testes\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "DYd10X5JoxsD",
    "outputId": "af5db3bf-f249-42b6-8370-d85b0738b9a0"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "#Visualizando Loss de treino e validação\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "# fazendo o plot do loss e validação\n",
    "# \"bo\" é o pontilhado azul\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b é a linha solida\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "pkh-ZmBCoyxX",
    "outputId": "fe18ef14-1099-4460-e6f0-0d667a3ce510"
   },
   "outputs": [],
   "source": [
    "#Visualizando Acuracia no treino e validação\n",
    "# \"bo\" é o pontilhado azul\n",
    "# b é a linha solida\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando novo modelo a partir do testado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FsnkTKsko0HH",
    "outputId": "3b83abc1-7304-479a-a079-3dbcdd687593"
   },
   "outputs": [],
   "source": [
    "#Criando um novo modelo para ser capaz de pegar uma string pura e fazer a avaliação\n",
    "#Pra isso, só adicionar a camada de normalização novamente e depois usar o modelo já treinado.\n",
    "export_model = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  model,\n",
    "  layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# testa com o dataset de teste só de strings\n",
    "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
    "print(accuracy)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FLh9tZNo2bn"
   },
   "outputs": [],
   "source": [
    "# Aqui é feita uma validação nossa\n",
    "examples = [\n",
    "  \"The movie was great!\",\n",
    "  \"The movie was okay.\",\n",
    "  \"The movie was terrible...\",\n",
    "  \"This movie was amazing\",\n",
    "  \"worst movie i ever seen\",\n",
    "  \"hated it\",\n",
    "  \"loved it\"\n",
    "]\n",
    "\n",
    "reviews2 = export_model.predict(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "llNgjh-Ao4Gj",
    "outputId": "a7dc187f-2528-421e-daca-20aabacd4829"
   },
   "outputs": [],
   "source": [
    "# mostrando os resultados\n",
    "for rev in reviews2:\n",
    "  if rev >=0.6:\n",
    "    print(f\"Bom | Nota: {rev}\")\n",
    "  elif rev<=0.4:\n",
    "    print(f\"Ruim | Nota: {rev}\")\n",
    "  else:\n",
    "    print(f\"Regular | Nota: {rev}\")  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvando o modelo no MONGODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebBr-cEbo5aW"
   },
   "outputs": [],
   "source": [
    "#Salva o modelo no formato h5 \n",
    "model.save('model.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4VnQ4CJo7CJ"
   },
   "outputs": [],
   "source": [
    "#Acesso ao bd da equipe\n",
    "cluster = MongoClient(\"mongodb+srv://yugiadm:yugi123@cluster0.ayqy5.mongodb.net/IAModels?retryWrites=true&w=majority\")\n",
    "db = cluster[\"IAModels\"]\n",
    "collection = db[\"modelos\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwuSWfNqo89a"
   },
   "outputs": [],
   "source": [
    "#Salvando no banco de dados\n",
    "#Importante alterar o nome do arquivo sempre que for salvar um modelo novo, para não dar problema no BD :D\n",
    "model_file = 'model.h5';\n",
    "with open(model_file, \"rb\") as f:\n",
    "    encoded = Binary(f.read())\n",
    "collection.insert_one({\"filename\": model_file, \"file\": encoded, \"description\": \"Keras model\" })\n",
    "\n",
    "# Retorna o modelo do banco\n",
    "data = collection.find_one({'filename': 'model.h5'})\n",
    "with open(\"keras_model_fromMongo.h5\", \"wb\") as f:\n",
    "    f.write(data['file'])\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Mp7JaKLo_Do"
   },
   "outputs": [],
   "source": [
    "#model_carregado recebe o modelo que foi salvo no BD\n",
    "model_carregado = keras.models.load_model('keras_model_fromMongo.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-KZ61ugsAeB",
    "outputId": "0d4a9dff-cdd9-4d5f-9629-b82e244995d5"
   },
   "outputs": [],
   "source": [
    "#Criando um outro modelo por motivos de testes e comparação\n",
    "#Esse modelo vai usar o modelo salvo no banco de dados\n",
    "export_model_2 = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  model_carregado,\n",
    "  layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model_2.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "loss, accuracy = export_model_2.evaluate(raw_test_ds)\n",
    "print(accuracy)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validação do modelo importado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gc-6iaVUo_9r"
   },
   "outputs": [],
   "source": [
    "#Realizando mais uma validação, mas com o modelo do banco\n",
    "examples = [\n",
    "  \"The movie is the greastest nicest and finest movie!\",\n",
    "  \"The movie was fine.\",\n",
    "  \"The movie was awful and trash...\",\n",
    "  \"wtf\",\n",
    "  \"worst movie i ever seen\",\n",
    "  \"hated it and hate it and hate it and hate it and hate it \",\n",
    "  \"loved it and loved it and loved it and loved it and loved it\"\n",
    "]\n",
    "\n",
    "reviews = export_model_2.predict(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_95j1jGpG_2",
    "outputId": "b82b433d-c614-4340-d91f-6304eee8d13a"
   },
   "outputs": [],
   "source": [
    "#Mostrando os resultados\n",
    "for rev in reviews:\n",
    "  if rev >=0.6:\n",
    "    print(f\"Bom | Nota: {rev}\")\n",
    "  elif rev<=0.4:\n",
    "    print(f\"Ruim | Nota: {rev}\")\n",
    "  else:\n",
    "    print(f\"Regular | Nota: {rev}\")  \n",
    " "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Agoravai2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
